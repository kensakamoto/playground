{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "形態素を表すクラスMorphを実装せよ．このクラスは表層形（surface），基本形（base），品詞（pos），品詞細分類1（pos1）をメンバ変数に持つこととする．さらに，CaboChaの解析結果（neko.txt.cabocha）を読み込み，各文をMorphオブジェクトのリストとして表現し，3文目の形態素列を表示せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Morph:\n",
    "    def __init__(self, surface, base, pos, pos1):\n",
    "        self.surface = surface\n",
    "        self.base = base\n",
    "        self.pos = pos\n",
    "        self.pos1 = pos1\n",
    "    def __str__(self):\n",
    "        return self.surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "sentences = []\n",
    "sentence = []\n",
    "for line in open('data/neko.txt.cabocha'):\n",
    "    x = re.search('(?P<surface>[^\\t]*?)\\t(?P<pos>[^,]*),(?P<pos1>[^,]*),[^,]*,[^,]*,[^,]*,[^,]*,(?P<base>[^,]*).*',line)\n",
    "    if x:\n",
    "        morph = Morph(x.group('surface'), x.group('base'), x.group('pos'), x.group('pos1'))\n",
    "#         print(morph)\n",
    "        sentence.append(morph)\n",
    "    elif line == 'EOS\\n' and not sentence == []:\n",
    "        sentences.append(sentence)\n",
    "        sentence = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['一'], ['\\u3000', '吾輩', 'は', '猫', 'で', 'ある', '。'], ['名前', 'は', 'まだ', '無い', '。'], ['\\u3000', 'どこ', 'で', '生れ', 'た', 'か', 'とんと', '見当', 'が', 'つか', 'ぬ', '。'], ['何', 'でも', '薄暗い', 'じめじめ', 'し', 'た', '所', 'で', 'ニャーニャー', '泣い', 'て', 'いた事', 'だけ', 'は', '記憶', 'し', 'て', 'いる', '。'], ['吾輩', 'は', 'ここ', 'で', '始め', 'て', '人間', 'という', 'もの', 'を', '見', 'た', '。'], ['しかも', 'あと', 'で', '聞く', 'と', 'それ', 'は', '書生', 'という', '人間', '中', 'で', '一番', '獰悪', 'な', '種族', 'で', 'あっ', 'た', 'そう', 'だ', '。'], ['この', '書生', 'という', 'の', 'は', '時々', '我々', 'を', '捕え', 'て', '煮', 'て', '食う', 'という', '話', 'で', 'ある', '。'], ['しかし', 'その', '当時', 'は', '何', 'という', '考', 'も', 'なかっ', 'た', 'から', '別段', '恐し', 'いとも', '思わ', 'なかっ', 'た', '。'], ['ただ', '彼', 'の', '掌', 'に', '載せ', 'られ', 'て', 'スー', 'と', '持ち上げ', 'られ', 'た', '時', '何だか', 'フワフワ', 'し', 'た', '感じ', 'が', 'あっ', 'た', 'ばかり', 'で', 'ある', '。']]\n"
     ]
    }
   ],
   "source": [
    "print([[str(y) for y in x] for x in sentences[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Chunk:\n",
    "    def __init__(self, morphs, dst, srcs):\n",
    "        self.morphs = morphs\n",
    "        self.dst = dst\n",
    "        self.srcs = srcs\n",
    "    def __str__(self):\n",
    "        ret = ''.join([str(x) for x in self.morphs])\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "sentences = []#sentence の list\n",
    "sentence = []#Chunk の list\n",
    "for line in open('data/neko.txt.cabocha'):\n",
    "    x = re.search('(?P<surface>.*?)\\t(?P<pos>[^,]*),(?P<pos1>[^,]*),[^,]*,[^,]*,[^,]*,[^,]*,(?P<base>[^,]*).*',line)\n",
    "    if x:\n",
    "        morph = Morph(x.group('surface'), x.group('base'), x.group('pos'), x.group('pos1'))\n",
    "        sentence[-1].morphs.append(morph)\n",
    "        continue\n",
    "    y = re.search('\\*\\ (?P<number>[0-9]+)\\ (?P<dst>-?[0-9]*)D.*', line)\n",
    "    if y:\n",
    "        dst = y.group('dst')\n",
    "        srcs = [i for i, j in enumerate(sentence) if j.dst == y.group('number')]\n",
    "        chunk = Chunk([], dst, srcs)\n",
    "        sentence.append(chunk)\n",
    "    elif line == 'EOS\\n' and not sentence == []:\n",
    "        sentences.append(sentence)\n",
    "        sentence = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['名前は->無い。', 'まだ->無い。', '無い。']\n"
     ]
    }
   ],
   "source": [
    "s = sentences[2]\n",
    "print([str(i) + '->' + str(s[int(i.dst)])  if i.dst != '-1' else str(i) for i in s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 42. 係り元と係り先の文節の表示\n",
    "係り元の文節と係り先の文節のテキストをタブ区切り形式ですべて抽出せよ．ただし，句読点などの記号は出力しないようにせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"data/file42.txt\", \"w\") as f:\n",
    "    for sentence in sentences:\n",
    "        for chunk in sentence:\n",
    "            p = [str(sentence[int(i)] for i in chunk.srcs)] + [str(chunk)]\n",
    "            p = p + [str(sentence[int(chunk.dst)])] if chunk.dst != '-1' else p\n",
    "            f.write('\\t'.join(list(p)) +  '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object <genexpr> at 0x112086f68>\t一\n",
      "<generator object <genexpr> at 0x112086fc0>\t　\t猫である。\n",
      "<generator object <genexpr> at 0x112086f10>\t吾輩は\t猫である。\n",
      "<generator object <genexpr> at 0x112086f10>\t猫である。\n",
      "<generator object <genexpr> at 0x112086fc0>\t名前は\t無い。\n",
      "<generator object <genexpr> at 0x112086f10>\tまだ\t無い。\n",
      "<generator object <genexpr> at 0x112086f10>\t無い。\n",
      "<generator object <genexpr> at 0x112086fc0>\t　どこで\t生れたか\n",
      "<generator object <genexpr> at 0x112086e60>\t生れたか\tつかぬ。\n",
      "<generator object <genexpr> at 0x112086eb8>\tとんと\tつかぬ。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('data/file42.txt', 'r') as r:\n",
    "    print(''.join(r.readlines()[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('data/file43.txt', 'a') as f:\n",
    "    for sentence in sentences:\n",
    "        for chunk in sentence:\n",
    "            if not [0 for i in chunk.morphs if i.pos == '名詞']:\n",
    "                continue\n",
    "            chun2 = sentence[int(chunk.dst)]\n",
    "            if not [0 for i in chun2.morphs if i.pos == '動詞']:\n",
    "                continue\n",
    "            f.write(str(chunk) + '\\t' + str(chun2) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "　どこで\t生れたか\n",
      "見当が\tつかぬ。\n",
      "所で\t泣いて\n",
      "ニャーニャー\t泣いて\n",
      "いた事だけは\t記憶している。\n",
      "記憶している。\t記憶している。\n",
      "吾輩は\t見た。\n",
      "ここで\t始めて\n",
      "ものを\t見た。\n",
      "あとで\t聞くと\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('data/file43.txt', 'r') as r:\n",
    "    print(''.join(r.readlines()[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 44. 係り受け木の可視化\n",
    "与えられた文の係り受け木を有向グラフとして可視化せよ．可視化には，係り受け木をDOT言語に変換し，Graphvizを用いるとよい．また，Pythonから有向グラフを直接的に可視化するには，pydotを使うとよい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "('requires pygraphviz ', 'http://pygraphviz.github.io/')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/Users/sakamoto/anaconda/lib/python3.6/site-packages/networkx/drawing/nx_agraph.py\u001b[0m in \u001b[0;36mto_agraph\u001b[0;34m(N)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0;32mimport\u001b[0m \u001b[0mpygraphviz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pygraphviz'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-e551ccd03239>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_edge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnx_agraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_agraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"data/file44.pdf\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pdf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dot'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sakamoto/anaconda/lib/python3.6/site-packages/networkx/drawing/nx_agraph.py\u001b[0m in \u001b[0;36mto_agraph\u001b[0;34m(N)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         raise ImportError('requires pygraphviz ',\n\u001b[0;32m--> 135\u001b[0;31m                           'http://pygraphviz.github.io/')\n\u001b[0m\u001b[1;32m    136\u001b[0m     \u001b[0mdirected\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_directed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber_of_selfloops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_multigraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: ('requires pygraphviz ', 'http://pygraphviz.github.io/')"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "s = sentences[4]\n",
    "G = nx.Graph()\n",
    "for chunk in s:\n",
    "    G.add_node(chunk, label=str(chunk))\n",
    "for chunk in s[:-1]:\n",
    "    G.add_edge(chunk, s[int(chunk.dst)])\n",
    "g = nx.nx_agraph.to_agraph(G)\n",
    "g.draw(f\"data/file44.pdf\",format='pdf', prog='dot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 45. 動詞の格パターンの抽出\n",
    "今回用いている文章をコーパスと見なし，日本語の述語が取りうる格を調査したい． 動詞を述語，動詞に係っている文節の助詞を格と考え，述語と格をタブ区切り形式で出力せよ． ただし，出力は以下の仕様を満たすようにせよ．\n",
    "\n",
    "動詞を含む文節において，最左の動詞の基本形を述語とする\n",
    "述語に係る助詞を格とする\n",
    "述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる\n",
    "「吾輩はここで始めて人間というものを見た」という例文（neko.txt.cabochaの8文目）を考える． この文は「始める」と「見る」の２つの動詞を含み，「始める」に係る文節は「ここで」，「見る」に係る文節は「吾輩は」と「ものを」と解析された場合は，次のような出力になるはずである．\n",
    "\n",
    "始める  で\n",
    "見る    は を\n",
    "このプログラムの出力をファイルに保存し，以下の事項をUNIXコマンドを用いて確認せよ．\n",
    "\n",
    "コーパス中で頻出する述語と格パターンの組み合わせ\n",
    "「する」「見る」「与える」という動詞の格パターン（コーパス中で出現頻度の高い順に並べよ）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/file45.txt', 'w') as f:\n",
    "    for sentence in sentences:\n",
    "        for chunk in sentence:\n",
    "            if chunk.morphs[-1].surface == '、':\n",
    "                chunk.morphs = chunk.morphs[:-1]\n",
    "            verbs = [i.base for i in chunk.morphs if i.pos == '動詞']\n",
    "            if verbs != []:\n",
    "                verb = verbs[0]\n",
    "                particles =  [sentence[i].morphs[-1].base for i in list(map(int,chunk.srcs)) if sentence[i].morphs[-1].pos == '助詞']\n",
    "                if particles != []:\n",
    "                    f.write(verb + '\\t' + ' '.join(particles) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 705 云う\tと\r\n",
      " 467 する\tを\r\n",
      " 343 思う\tと\r\n",
      " 206 ある\tが\r\n",
      " 203 なる\tに\r\n",
      " 195 する\tに\r\n",
      " 182 見る\tて\r\n",
      " 161 する\tと\r\n",
      " 118 する\tが\r\n",
      " 100 見る\tを\r\n",
      "sort: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "!sort data/file45.txt | uniq -c | sort -r | head -n 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   4 与える\tに を\r\n",
      "   2 与える\tて に を\r\n",
      "   1 与える\tけれども は を\r\n",
      "   1 与える\tとして か\r\n",
      "   1 与える\tが は と て に を\r\n",
      "   1 与える\tは て に を に\r\n",
      "   1 与える\tは て に を\r\n",
      "   1 与える\tと は て を\r\n",
      "   1 与える\tて は に を\r\n",
      "   1 与える\tは は も\r\n",
      "   1 与える\tで に を\r\n",
      "   1 与える\tも を\r\n",
      "   1 与える\tば を\r\n",
      "   1 与える\tが を\r\n"
     ]
    }
   ],
   "source": [
    "!grep  '^与える\\t' data/file45.txt | sort | uniq -c | sort -r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 46. 動詞の格フレーム情報の抽出\n",
    "45のプログラムを改変し，述語と格パターンに続けて項（述語に係っている文節そのもの）をタブ区切り形式で出力せよ．45の仕様に加えて，以下の仕様を満たすようにせよ．\n",
    "\n",
    "項は述語に係っている文節の単語列とする（末尾の助詞を取り除く必要はない）\n",
    "述語に係る文節が複数あるときは，助詞と同一の基準・順序でスペース区切りで並べる 「吾輩はここで始めて人間というものを見た」という例文（neko.txt.cabochaの8文目）を考える． この文は「始める」と「見る」の２つの動詞を含み，「始める」に係る文節は「ここで」，「見る」に係る文節は「吾輩は」と「ものを」と解析された場合は，次のような出力になるはずである．\n",
    "\n",
    "始める で ここで\n",
    "\n",
    "見る は を 吾輩は ものを"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/file46.txt', 'w') as f:\n",
    "    for sentence in sentences:\n",
    "        for chunk in sentence:\n",
    "            if chunk.morphs[-1].surface == '、':\n",
    "                chunk.morphs = chunk.morphs[:-1]\n",
    "            verbs = [i.base for i in chunk.morphs if i.pos == '動詞']\n",
    "            if verbs != []:\n",
    "                verb = verbs[0]\n",
    "                units = [sentence[i] for i in list(map(int,chunk.srcs)) if sentence[i].morphs[-1].pos == '助詞']\n",
    "                particles = [i.morphs[-1].base for i in units]\n",
    "                if particles != []:\n",
    "                    f.write(verb + '\\t' + ' '.join(particles) + '\\t' + ' '.join([str(i) for i in units]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生れる\tで\t　どこで\n",
      "つく\tか が\t生れたか 見当が\n",
      "泣く\tで\t所で\n",
      "する\tて は\t泣いて いた事だけは\n",
      "始める\tで\tここで\n",
      "見る\tは を\t吾輩は ものを\n",
      "聞く\tで\tあとで\n",
      "捕える\tを\t我々を\n",
      "煮る\tて\t捕えて\n",
      "食う\tて\t煮て\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for line in open('data/file46.txt'):\n",
    "    print(line, end='')\n",
    "    i = i + 1\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
